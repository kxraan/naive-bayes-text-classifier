{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytESVPElL6We",
        "outputId": "e1b91b1e-c64c-4b4e-9c71-4d797dbc49e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "src_path = '/content/drive/My\\ Drive/CMSC422/HW1/20_newsgroups.tar.gz'\n",
        "extract_path = '/content/20_newsgroups'\n",
        "\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "!tar -xzf {src_path} -C {extract_path}\n"
      ],
      "metadata": {
        "id": "Ygeyi5CyNlHS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/20_newsgroups/20_newsgroups'\n",
        "\n",
        "newsgroup_classes = os.listdir(dataset_path)\n",
        "print(\"Newsgroup Categories:\", newsgroup_classes)\n",
        "\n",
        "sample_class_path = os.path.join(dataset_path, newsgroup_classes[0])\n",
        "sample_files = os.listdir(sample_class_path)\n",
        "print(\"\\nSample files in category '{}':\".format(newsgroup_classes[0]), sample_files[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oH-daN-SGTf",
        "outputId": "0036bd45-84dd-43d3-b638-e168990e8ade"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Newsgroup Categories: ['talk.politics.mideast', 'comp.sys.mac.hardware', 'comp.os.ms-windows.misc', 'talk.politics.misc', 'rec.sport.baseball', 'comp.windows.x', 'rec.sport.hockey', 'sci.electronics', 'comp.sys.ibm.pc.hardware', 'rec.autos', 'sci.space', 'talk.politics.guns', 'misc.forsale', 'talk.religion.misc', 'soc.religion.christian', 'rec.motorcycles', 'sci.med', 'alt.atheism', 'sci.crypt', 'comp.graphics']\n",
            "\n",
            "Sample files in category 'talk.politics.mideast': ['76346', '76416', '77263', '76376', '76331']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print out to see how the data looks\n",
        "sample_class_path = os.path.join(dataset_path, newsgroup_classes[0])\n",
        "sample_files = os.listdir(sample_class_path)\n",
        "sample_file_path = os.path.join(sample_class_path, sample_files[0])\n",
        "\n",
        "\n",
        "with open(sample_file_path, 'r', encoding='latin1') as file:\n",
        "    lines = file.readlines()\n",
        "    print(\"\\nFirst 10 lines of '{}':\".format(sample_files[0]))\n",
        "    for line in lines[:30]:\n",
        "        print(line.strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onyIoIDpSXxR",
        "outputId": "3d90c61f-1858-41bc-9377-00d013cd3deb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 lines of '76346':\n",
            "Path: cantaloupe.srv.cs.cmu.edu!crabapple.srv.cs.cmu.edu!fs7.ece.cmu.edu!europa.eng.gtefsd.com!howland.reston.ans.net!zaphod.mps.ohio-state.edu!cs.utexas.edu!sdd.hp.com!sgiblab!sgigate!sgi!cdp!cpr\n",
            "From: Center for Policy Research <cpr@igc.apc.org>\n",
            "Newsgroups: talk.politics.mideast\n",
            "Date: 22 Apr 93 17:31 PDT\n",
            "Subject: Re: rejoinder. Questions to Israelis\n",
            "Message-ID: <1483500353@igc.apc.org>\n",
            "References: <1483500352@igc.apc.org>\n",
            "Sender: Notesfile to Usenet Gateway <notes@igc.apc.org>\n",
            "Nf-ID: #R:cdp:1483500352:cdp:1483500353:000:3689\n",
            "Nf-From: cdp.UUCP!cpr    Apr 22 17:31:00 1993\n",
            "Lines: 83\n",
            "\n",
            "\n",
            "From: Center for Policy Research <cpr>\n",
            "Subject: rejoinder. Questions to Israelis\n",
            "\n",
            "\n",
            "Dear Josh\n",
            "\n",
            "I appreciate the fact that you sought to answer my questions.\n",
            "\n",
            "Having said that, I am not totally happy with your answers.\n",
            "\n",
            "1.   You did not fully answer my question whether Israeli ID cards\n",
            "identify the holders as Jews or Arabs. You imply that U.S.\n",
            "citizens must identify themselves by RACE. Is that true ? Or are\n",
            "just trying to mislead the reader ? Do you know of any democratic\n",
            "country where people are asked to reveal their ethnical or\n",
            "religious identity to any public official who so requests ?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "def process_all_files(dataset_path):\n",
        "\n",
        "    all_words = []\n",
        "    cleaned_data = {}\n",
        "\n",
        "    for newsgroup in os.listdir(dataset_path):\n",
        "        newsgroup_path = os.path.join(dataset_path, newsgroup)\n",
        "        if os.path.isdir(newsgroup_path):\n",
        "            cleaned_data[newsgroup] = []\n",
        "\n",
        "            for filename in os.listdir(newsgroup_path):\n",
        "                file_path = os.path.join(newsgroup_path, filename)\n",
        "\n",
        "                with open(file_path, 'r', encoding='latin1') as file:\n",
        "                    lines = file.readlines()\n",
        "\n",
        "                # Remove the first four lines\n",
        "                content = ''.join(lines[4:])\n",
        "\n",
        "                # Convert to lowercase\n",
        "                content = content.lower()\n",
        "\n",
        "                # Remove punctuation\n",
        "                content = content.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "                # Split into words\n",
        "                words = content.split()\n",
        "\n",
        "                # Collect words for stop word calculation\n",
        "                all_words.extend(words)\n",
        "\n",
        "\n",
        "                cleaned_data[newsgroup].append((words, file_path))\n",
        "\n",
        "    # Finding the top 200 words as stop words\n",
        "    word_counter = Counter(all_words)\n",
        "    top_200_stop_words = set([word for word, _ in word_counter.most_common(200)])\n",
        "\n",
        "    final_cleaned_data = {}\n",
        "    for newsgroup, files in cleaned_data.items():\n",
        "        final_cleaned_data[newsgroup] = []\n",
        "\n",
        "        for words, file_path in files:\n",
        "            # Remove stop words, short words, and email addresses\n",
        "            words_filtered = [\n",
        "                word for word in words\n",
        "                if word not in top_200_stop_words and\n",
        "                len(word) > 2 and\n",
        "                '@' not in word\n",
        "            ]\n",
        "\n",
        "            # Join filtered words\n",
        "            cleaned_content = ' '.join(words_filtered)\n",
        "\n",
        "            final_cleaned_data[newsgroup].append((cleaned_content, file_path))\n",
        "\n",
        "    return final_cleaned_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OxRhFEa_WRaW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_dataset = process_all_files(dataset_path)\n",
        "\n",
        "# Display a cleaned file\n",
        "example_class = os.listdir(dataset_path)[0]\n",
        "cleaned_content, file_path = cleaned_dataset[example_class][0]\n",
        "\n",
        "print(f\"\\nSample cleaned email from '{example_class}':\\n\")\n",
        "print(f\"File Path: {file_path}\")\n",
        "print(f\"Content: {cleaned_content[:500]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7pwX4GgY6E7",
        "outputId": "3ad2a55f-53f5-419c-f797-4a50490a510d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample cleaned email from 'talk.politics.mideast':\n",
            "\n",
            "File Path: /content/20_newsgroups/20_newsgroups/talk.politics.mideast/76346\n",
            "Content: rejoinder questions israelis 1483500353igcapcorg 1483500352igcapcorg notesfile gateway notesigcapcorg nfid rcdp1483500352cdp14835003530003689 nffrom cdpuucpcpr 173100 center policy research cpr rejoinder questions israelis dear josh appreciate fact sought answer questions having totally happy answers fully answer whether israeli cards identify holders jews arabs imply citizens identify themselves race true trying mislead reader democratic country asked reveal ethnical religious identity public o\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Spliting the datasets\n",
        "\n",
        "\n",
        "train ={}\n",
        "test = {}\n",
        "\n",
        "for category, documents in cleaned_dataset.items():\n",
        "  train[category] = documents[:500]\n",
        "  test[category] = documents[500:]\n",
        "\n",
        "\n",
        "example_class = list(cleaned_dataset.keys())[0]\n",
        "print(f\"Number of documents in '{example_class}' - Training: {len(train[example_class])}, Testing: {len(test[example_class])}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hFyfR5bU972h",
        "outputId": "967c3566-572a-48d2-8499-26b00cf2d123"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents in 'talk.politics.mideast' - Training: 500, Testing: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training\n",
        "import math\n",
        "from collections import defaultdict\n",
        "\n",
        "Ndoc = 500*20\n",
        "Cdoc = 500\n",
        "bigdoc = defaultdict(Counter)\n",
        "V = set()\n",
        "logprior = math.log(Cdoc / Ndoc)\n",
        "loglikelihood = defaultdict(dict)\n",
        "\n",
        "\n",
        "for c, doc in train.items():\n",
        "\n",
        "\n",
        "  words_count= []\n",
        "\n",
        "  for content, _ in doc:\n",
        "    words = content.split()\n",
        "    words_count.extend(words)\n",
        "    V.update(words)\n",
        "\n",
        "  bigdoc[c] = Counter(words_count)\n",
        "\n",
        "for c in train:\n",
        "\n",
        "  total_words = sum(bigdoc[c].values())\n",
        "\n",
        "  for words in V:\n",
        "    words_count = bigdoc[c][words] +1\n",
        "    deno = total_words + len(V)\n",
        "    loglikelihood[c][words] = math.log(words_count/ deno)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5CNlbsrWBi0R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing\n",
        "\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "\n",
        "for c, doc in test.items():\n",
        "\n",
        "  score = {}\n",
        "  for content, _ in doc:\n",
        "\n",
        "    for class_label in loglikelihood:\n",
        "      score[class_label] = logprior\n",
        "\n",
        "    words = content.split()\n",
        "    word_count_in_vocab = sum(1 for w in words if w in V)\n",
        "\n",
        "    for w in words:\n",
        "      if w in V:\n",
        "\n",
        "         for class_label in loglikelihood:\n",
        "          score[class_label] += loglikelihood[class_label].get(w, 0)\n",
        "\n",
        "\n",
        "\n",
        "    best_class = max(score, key= score.get)\n",
        "\n",
        "    if best_class == c:\n",
        "      correct_predictions +=1\n",
        "    total_predictions +=1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions * 100\n",
        "print(f\"Model accuracy on test data: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_hhReLVHYu_",
        "outputId": "0556f2fe-6576-4ec9-f87f-05bd3f288bce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy on test data: 83.61\n"
          ]
        }
      ]
    }
  ]
}